{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Data**\n"
      ],
      "metadata": {
        "id": "0vWhBdvaU8-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain data from yfinance\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the list of top 100 company tickers, use only AAPL for now\n",
        "tickers = ['AAPL']\n",
        "\n",
        "# Set the date range\n",
        "start_date = '1994-11-06'\n",
        "end_date = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "# Loop through each ticker and download data\n",
        "for ticker in tickers:\n",
        "    # Download historical data\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    # Reset column names to avoid mismatched headers\n",
        "    data.columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "    # Reset the index to a new \"Date\" column and keep only the date part\n",
        "    data.reset_index(inplace=True)\n",
        "    data['Date'] = data['Date'].dt.date  # Convert DateTime to just date (YYYY-MM-DD format) to get rid of hours, minutes, and seconds\n",
        "    data['Date'] = pd.to_datetime(data['Date']) # Convert 'Date' column to datetime format so model interpre it as dates\n",
        "\n",
        "\n",
        "    # Save to CSV with Date as a column\n",
        "    data.to_csv(f'{ticker}_30yrs.csv', index=False)  # Save without the index to make Date a column\n",
        "print('\\n')\n",
        "# print(data.tail())\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_RyYaPMZ3HU",
        "outputId": "90f518db-7d9f-46c3-c4c8-bf4315ea01d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "        Date  Adj Close     Close      High       Low      Open     Volume\n",
            "0 1994-11-07   0.302541  0.363839  0.368304  0.358259  0.360491  113041600\n",
            "1 1994-11-08   0.313678  0.377232  0.380580  0.359375  0.362723  348969600\n",
            "2 1994-11-09   0.309038  0.371652  0.383929  0.366071  0.381696  406336000\n",
            "3 1994-11-10   0.306718  0.368862  0.373884  0.366071  0.372768  152980800\n",
            "4 1994-11-11   0.305326  0.367188  0.370536  0.366071  0.368304   62272000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build and fit LSTM model**"
      ],
      "metadata": {
        "id": "VBZk8ULn3yUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Load data and convert 'Date' to datetime format\n",
        "data = pd.read_csv('AAPL_30yrs.csv')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Use only the 'Close' column for prediction\n",
        "close_prices = data[['Close']]\n",
        "\n",
        "# Standardize the close prices for better training performance\n",
        "scaler = StandardScaler()\n",
        "close_prices_scaled = scaler.fit_transform(close_prices)\n",
        "\n",
        "# Define sequence length (e.g., 60 days of historical prices)\n",
        "sequence_length = 60\n",
        "\n",
        "# Create sequences and targets for the model\n",
        "X = []\n",
        "y = []\n",
        "for i in range(sequence_length, len(close_prices_scaled)):\n",
        "    X.append(close_prices_scaled[i-sequence_length:i])  # Last 60 days\n",
        "    y.append(close_prices_scaled[i, 0])  # Next day's price\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Reshape X to (samples, time steps, features) for LSTM input\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True),\n",
        "    LSTM(50),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Reverse the standardization to get predictions in original scale\n",
        "y_test_unscaled = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "y_pred_unscaled = scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "# Plot the actual vs. predicted values on test data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['Date'][-len(y_test):], y_test_unscaled, label='Actual Closing Price', color='blue')\n",
        "plt.plot(data['Date'][-len(y_test):], y_pred_unscaled, label='Predicted Closing Price', color='green')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price in US dollars ($)')\n",
        "plt.title(\"Actual vs Predicted Closing Prices of Apple\")\n",
        "plt.legend()\n",
        "\n",
        "# Set date format on x-axis\n",
        "plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RXVAXnQW32i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a995e3f-9fed-4894-8d3e-9a0716d4a36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - loss: 0.0509 - val_loss: 0.1815\n",
            "Epoch 2/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - loss: 0.0011 - val_loss: 0.0529\n",
            "Epoch 3/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 65ms/step - loss: 9.3219e-04 - val_loss: 0.0524\n",
            "Epoch 4/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 80ms/step - loss: 7.9520e-04 - val_loss: 0.0490\n",
            "Epoch 5/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - loss: 7.2520e-04 - val_loss: 0.0450\n",
            "Epoch 6/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - loss: 5.3939e-04 - val_loss: 0.0301\n",
            "Epoch 7/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - loss: 6.1817e-04 - val_loss: 0.0604\n",
            "Epoch 8/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 66ms/step - loss: 6.4751e-04 - val_loss: 0.0196\n",
            "Epoch 9/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 65ms/step - loss: 4.9063e-04 - val_loss: 0.0191\n",
            "Epoch 10/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - loss: 4.7217e-04 - val_loss: 0.0110\n",
            "Epoch 11/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 65ms/step - loss: 4.3403e-04 - val_loss: 0.0257\n",
            "Epoch 12/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 64ms/step - loss: 3.9376e-04 - val_loss: 0.0165\n",
            "Epoch 13/20\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - loss: 4.4211e-04 - val_loss: 0.0158\n",
            "Epoch 14/20\n"
          ]
        }
      ]
    }
  ]
}