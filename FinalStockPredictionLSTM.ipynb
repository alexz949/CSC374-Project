{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection and Create CSV file**\n"
      ],
      "metadata": {
        "id": "0vWhBdvaU8-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def download_stock_data(tickers, start_date, end_date=datetime.today().strftime('%Y-%m-%d')):\n",
        "    # Loop through each ticker and download data\n",
        "    for ticker in tickers:\n",
        "        # Download historical data\n",
        "        data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        # Reset column names to avoid mismatched headers\n",
        "        data.columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "        # Reset the index to a new \"Date\" column and keep only the date part\n",
        "        data.reset_index(inplace=True)\n",
        "        data['Date'] = data['Date'].dt.date  # Convert DateTime to just date (YYYY-MM-DD format) to get rid of hours, minutes, and seconds\n",
        "        data['Date'] = pd.to_datetime(data['Date']) # Convert 'Date' column to datetime format so model interpre it as dates\n",
        "\n",
        "        # Save to CSV with Date as a column\n",
        "        data.to_csv(f'{ticker}_10yrs.csv', index=False)  # Save without the index to make Date a column"
      ],
      "metadata": {
        "id": "P_RyYaPMZ3HU"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM model Implementation**"
      ],
      "metadata": {
        "id": "VBZk8ULn3yUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation and Scaling**"
      ],
      "metadata": {
        "id": "hsiII_T15xh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_stock_data(ticker):\n",
        "    #  Load data and convert 'Date' to datetime format\n",
        "    data = pd.read_csv(f'{ticker}_10yrs.csv')\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "    # Use only the 'Close' column for prediction\n",
        "    close_prices = data[['Close']]\n",
        "\n",
        "    # Standardize the close prices for better training performance\n",
        "    scaler = StandardScaler()\n",
        "    close_prices_scaled = scaler.fit_transform(close_prices)\n",
        "\n",
        "    return data, close_prices_scaled, scaler"
      ],
      "metadata": {
        "id": "vOOOHDh050Gz"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence Creation**"
      ],
      "metadata": {
        "id": "AQTmFTzI506T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_data_for_lstm(close_prices_scaled, sequence_length=60):\n",
        "    # Define sequence length (60 days of historical prices)\n",
        "\n",
        "    # Create sequences and targets for the model\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(sequence_length, len(close_prices_scaled)):\n",
        "        X.append(close_prices_scaled[i-sequence_length:i])  # Last 30 days\n",
        "        y.append(close_prices_scaled[i, 0])  # Next day's price\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape X to (samples, time steps, features) for LSTM input\n",
        "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "    # Split the data into training and testing sets, remain chronological order\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Show input data details\n",
        "    print(f\"X shape and y shape: {X.shape, y.shape}\")\n",
        "    print(f\"X_train shape and y_train shape: {X_train.shape, y_train.shape}\")\n",
        "    print(f\"X_test shape and y_test shape: {X_test.shape, y_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "e0UNrFCN6QRK"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build and Train Model**"
      ],
      "metadata": {
        "id": "n7o5fB2n6Rf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def show_model_info(model):\n",
        "    # Show model summary\n",
        "    print(model.summary())\n",
        "    return\n",
        "\n",
        "def build_and_train_model(X_train, y_train, X_test, y_test, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape):\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the scaler on the training data\n",
        "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n",
        "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "    # Transform the test data using the fitted scaler\n",
        "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n",
        "    y_test_scaled = scaler.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Reshape scaled data back to the original shape for LSTM\n",
        "    X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
        "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
        "\n",
        "    model = Sequential([\n",
        "        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(dropout_rate),  # Dropout rate of 20%\n",
        "        LSTM(lstm_units),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=learning_rate)  # Adjust learning rate\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Train the model with verbose set to 0 to suppress epoch output\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train_scaled,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_test_scaled, y_test_scaled),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred_scaled = model.predict(X_test_scaled)\n",
        "\n",
        "    # Reverse the standardization to get predictions in original scale\n",
        "    y_test_unscaled = scaler.inverse_transform(y_test_scaled).flatten()\n",
        "    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "    return model, history, y_pred_unscaled, y_test_unscaled\n"
      ],
      "metadata": {
        "id": "IaAJIU5q6UfD"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare for Incremental Learning**"
      ],
      "metadata": {
        "id": "c7foCBRyJ7vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def get_new_data(ticker, scaler, sequence_length=60):\n",
        "    # scaler = StandardScaler()\n",
        "    end_date = datetime.today().strftime('%Y-%m-%d')\n",
        "    start_date = (datetime.today() - timedelta(days=90)).strftime('%Y-%m-%d')\n",
        "\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    data.columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "    # Convert to DataFrame to preserve feature names\n",
        "    close_prices_df = pd.DataFrame(data['Close'])\n",
        "\n",
        "    # Scale and fit data\n",
        "    close_prices_scaled = scaler.fit_transform(close_prices_df)\n",
        "    X_new = []\n",
        "    y_new = []\n",
        "    for i in range(sequence_length, len(close_prices_scaled)):\n",
        "        X_new.append(close_prices_scaled[i-sequence_length:i])\n",
        "        y_new.append(close_prices_scaled[i, 0])  # Next day's price\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_new, y_new = np.array(X_new), np.array(y_new)\n",
        "    y_new = y_new.reshape(-1, 1)\n",
        "\n",
        "    return X_new, y_new\n"
      ],
      "metadata": {
        "id": "JCrRyb8kJ96b"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Incremental Learning**"
      ],
      "metadata": {
        "id": "Y4KOmjAekvQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def incremental_learning_process(ticker, model, scaler, iterations=10):\n",
        "    for epoch in range(1, iterations + 1):\n",
        "        X_new, y_new = get_new_data(ticker, scaler)\n",
        "\n",
        "        # Reshape for model input\n",
        "        X_new_reshaped = X_new.reshape(X_new.shape[0], X_new.shape[1], 1)\n",
        "\n",
        "        # Train the model with verbose=0 to silence epoch printing\n",
        "        model.fit(X_new_reshaped, y_new, epochs=5, batch_size=64, verbose=0)\n",
        "\n",
        "        # Predict on the new data\n",
        "        y_pred_scaled = model.predict(X_new_reshaped)\n",
        "\n",
        "        # Inverse transform to get original scale\n",
        "        y_pred_unscaled = scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "        # Optionally print a summary for each update\n",
        "        print(f'Update {epoch}: predicted price:', y_pred_unscaled)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "q7Qd_Wq2k20T"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics Evaluation**"
      ],
      "metadata": {
        "id": "DYZBMeli7zVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def evaluate_model_performance(ticker, y_test_unscaled, y_pred_unscaled):\n",
        "    # RMSE (Root Mean Square Error)\n",
        "    print(f'For {ticker}:')\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "\n",
        "    # MAE (Mean Absolute Error)\n",
        "    mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
        "    print(f\"MAE: {mae}\")\n",
        "\n",
        "    # Direction Accuracy\n",
        "    direction_accuracy = np.mean(\n",
        "        np.sign(y_test_unscaled[1:] - y_test_unscaled[:-1]) == np.sign(y_pred_unscaled[1:] - y_test_unscaled[:-1])\n",
        "    ) * 100\n",
        "    print(f\"Direction Accuracy: {direction_accuracy:.2f}%\")\n",
        "\n",
        "    # Backtesting\n",
        "    # Simulate trading: Buy if price is predicted to go up, sell if it goes down\n",
        "    returns = (y_test_unscaled[1:] - y_test_unscaled[:-1])  # Actual price changes\n",
        "    predicted_returns = (y_pred_unscaled[1:] - y_test_unscaled[:-1])  # Predicted price changes\n",
        "    profit = np.sum(np.sign(predicted_returns) * returns)\n",
        "\n",
        "    print(f\"Backtesting Profit: ${profit:.2f}\")"
      ],
      "metadata": {
        "id": "xhOFjsF471SV"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot**\n"
      ],
      "metadata": {
        "id": "_qheOtQ446bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "def plot_actual_vs_predicted(data, y_test_unscaled, y_pred_unscaled, ticker):\n",
        "    # Plot the actual vs. predicted values on test data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plotting only the test range\n",
        "    plt.plot(data['Date'][-len(y_test_unscaled):], y_test_unscaled, label='Actual Closing Price', color='blue')\n",
        "    plt.plot(data['Date'][-len(y_test_unscaled):], y_pred_unscaled, label='Predicted Closing Price', color='green')\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Closing Price reverse z-score normalization')\n",
        "    plt.title(f\"Actual vs Predicted Closing Prices of {ticker} (2 Years)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Set date format on x-axis (adapted for 10-year range)\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))  # Tick every year\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Format ticks as 'YYYY'\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OXx87CKM45Iw"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function**"
      ],
      "metadata": {
        "id": "Bxj7HE4SUdXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tickers and start date, end date is default today\n",
        "# tickers = ['AAPL']\n",
        "tickers = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOG', '2222.SR', 'META', 'TSLA', 'TSM', 'AVGO']\n",
        "\n",
        "start_date = '2014-9-11'\n",
        "lstm_units = 50\n",
        "dropout_rate=0.2\n",
        "learning_rate=0.01\n",
        "epochs=20\n",
        "batch_size=64\n",
        "sequence_length=60 # Sequence length is consistent with the input shape\n",
        "input_shape=(60, 1)\n",
        "\n",
        "def lstm_model(ticker, start_date, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape):\n",
        "    # Download the data from yfinace\n",
        "    download_stock_data(tickers, start_date)\n",
        "    # print(data.tail())\n",
        "    # print(data.head())\n",
        "\n",
        "    # Scale data\n",
        "    data, close_prices_scaled, scaler = scale_stock_data(ticker)\n",
        "\n",
        "    # Create sequence for lstm\n",
        "    X_train, X_test, y_train, y_test = prepare_data_for_lstm(close_prices_scaled, sequence_length)\n",
        "\n",
        "    # Define and build the model\n",
        "    model, history, y_pred_unscaled, y_test_unscaled = build_and_train_model(X_train, y_train, X_test, y_test, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape)\n",
        "\n",
        "    # Show model information\n",
        "    show_model_info(model)\n",
        "\n",
        "    # Incremental Learning\n",
        "    updated_model = incremental_learning_process(ticker, model, scaler)\n",
        "\n",
        "\n",
        "    # Evaluate model\n",
        "    evaluate_model_performance(ticker, y_test_unscaled, y_pred_unscaled)\n",
        "\n",
        "    # Plot the model\n",
        "    plot_actual_vs_predicted(data, y_test_unscaled, y_pred_unscaled, ticker)\n",
        "    return\n",
        "\n",
        "for ticker in tickers:\n",
        "    lstm_model(ticker, start_date, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4o8sxAuUglR",
        "outputId": "33f8d3ee-d9c2-4ae7-811c-ae8e8d2bc8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape and y shape: ((2523, 60, 1), (2523,))\n",
            "X_train shape and y_train shape: ((2018, 60, 1), (2018,))\n",
            "X_test shape and y_test shape: ((505, 60, 1), (505,))\n"
          ]
        }
      ]
    }
  ]
}