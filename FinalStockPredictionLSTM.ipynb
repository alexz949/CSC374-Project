{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection and Create CSV file**\n"
      ],
      "metadata": {
        "id": "0vWhBdvaU8-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def download_stock_data(tickers, start_date, end_date=datetime.today().strftime('%Y-%m-%d')):\n",
        "    # Loop through each ticker and download data\n",
        "    for ticker in tickers:\n",
        "        # Download historical data\n",
        "        data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        # Reset column names to avoid mismatched headers\n",
        "        data.columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "        # Reset the index to a new \"Date\" column and keep only the date part\n",
        "        data.reset_index(inplace=True)\n",
        "        data['Date'] = data['Date'].dt.date  # Convert DateTime to just date (YYYY-MM-DD format) to get rid of hours, minutes, and seconds\n",
        "        data['Date'] = pd.to_datetime(data['Date']) # Convert 'Date' column to datetime format so model interpre it as dates\n",
        "\n",
        "        # Save to CSV with Date as a column\n",
        "        data.to_csv(f'{ticker}_10yrs.csv', index=False)  # Save without the index to make Date a column"
      ],
      "metadata": {
        "id": "P_RyYaPMZ3HU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM model Implementation**"
      ],
      "metadata": {
        "id": "VBZk8ULn3yUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation and Scaling**"
      ],
      "metadata": {
        "id": "hsiII_T15xh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_stock_data(ticker):\n",
        "    #  Load data and convert 'Date' to datetime format\n",
        "    data = pd.read_csv(f'{ticker}_10yrs.csv')\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "    # Use only the 'Close' column for prediction\n",
        "    close_prices = data[['Close']]\n",
        "\n",
        "    # Standardize the close prices for better training performance\n",
        "    scaler = StandardScaler()\n",
        "    close_prices_scaled = scaler.fit_transform(close_prices)\n",
        "\n",
        "    return data, close_prices_scaled, scaler"
      ],
      "metadata": {
        "id": "vOOOHDh050Gz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence Creation**"
      ],
      "metadata": {
        "id": "AQTmFTzI506T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_data_for_lstm(close_prices_scaled, sequence_length=60):\n",
        "    # Define sequence length (60 days of historical prices)\n",
        "\n",
        "    # Create sequences and targets for the model\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(sequence_length, len(close_prices_scaled)):\n",
        "        X.append(close_prices_scaled[i-sequence_length:i])  # Last 30 days\n",
        "        y.append(close_prices_scaled[i, 0])  # Next day's price\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape X to (samples, time steps, features) for LSTM input\n",
        "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "    # Split the data into training and testing sets, remain chronological order\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Show input data details\n",
        "    print(f\"X shape and y shape: {X.shape, y.shape}\")\n",
        "    print(f\"X_train shape and y_train shape: {X_train.shape, y_train.shape}\")\n",
        "    print(f\"X_test shape and y_test shape: {X_test.shape, y_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "e0UNrFCN6QRK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build and Train Model**"
      ],
      "metadata": {
        "id": "n7o5fB2n6Rf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def show_model_info(model):\n",
        "    # Show model summary\n",
        "    print(model.summary())\n",
        "    return\n",
        "\n",
        "def build_and_train_model(X_train, y_train, X_test, y_test, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape):\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the scaler on the training data\n",
        "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n",
        "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "    # Transform the test data using the fitted scaler\n",
        "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n",
        "    y_test_scaled = scaler.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Reshape scaled data back to the original shape for LSTM\n",
        "    X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
        "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
        "\n",
        "    model = Sequential([\n",
        "        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(dropout_rate),  # Dropout rate of 20%\n",
        "        LSTM(lstm_units),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=learning_rate)  # Adjust learning rate\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Train the model with verbose set to 0 to suppress epoch output\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train_scaled,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_test_scaled, y_test_scaled),\n",
        "        verbose=0  # Change this to 1 for a progress bar or 2 for one line per epoch\n",
        "    )\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred_scaled = model.predict(X_test_scaled)\n",
        "\n",
        "    # Reverse the standardization to get predictions in original scale\n",
        "    y_test_unscaled = scaler.inverse_transform(y_test_scaled).flatten()\n",
        "    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "    return model, history, y_pred_unscaled, y_test_unscaled\n"
      ],
      "metadata": {
        "id": "IaAJIU5q6UfD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare for Incremental Learning**"
      ],
      "metadata": {
        "id": "c7foCBRyJ7vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def get_new_data(ticker, scaler, sequence_length=60):\n",
        "    end_date = datetime.today().strftime('%Y-%m-%d')\n",
        "    start_date = (datetime.today() - timedelta(days=90)).strftime('%Y-%m-%d')\n",
        "\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    data.columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "    # Convert to DataFrame to preserve feature names\n",
        "    close_prices_df = pd.DataFrame(data['Close'])\n",
        "\n",
        "    # Fit scaler if not already fitted\n",
        "    if not hasattr(scaler, 'mean_'):\n",
        "        scaler.fit(close_prices_df)\n",
        "\n",
        "    close_prices_scaled = scaler.transform(close_prices_df)\n",
        "\n",
        "    X_new = np.array([close_prices_scaled[i-sequence_length:i]\n",
        "                      for i in range(sequence_length, len(close_prices_scaled))])\n",
        "    y_new = close_prices_scaled[sequence_length:, 0]\n",
        "\n",
        "    return X_new, y_new\n",
        "\n",
        "# def create_lstm_model(input_shape=(30, 1)):\n",
        "#     model = Sequential([\n",
        "#         LSTM(50, return_sequences=True, input_shape=input_shape),\n",
        "#         Dropout(0.2),\n",
        "#         LSTM(50),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(1)\n",
        "#     ])\n",
        "\n",
        "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "#     return model"
      ],
      "metadata": {
        "id": "JCrRyb8kJ96b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Incremental Learning**"
      ],
      "metadata": {
        "id": "Y4KOmjAekvQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def incremental_learning_process(ticker, model, scaler, iterations=10):\n",
        "    for epoch in range(1, iterations + 1):\n",
        "        X_new, y_new = get_new_data(ticker, scaler)\n",
        "\n",
        "        print(\"shape\\n\")\n",
        "        print(X_new.shape)\n",
        "        print(y_new.shape)\n",
        "\n",
        "        # Reshape for model input\n",
        "        X_new_reshaped = X_new.reshape(X_new.shape[0], X_new.shape[1], 1)\n",
        "\n",
        "        # Train the model with verbose=0 to silence epoch printing\n",
        "        model.fit(X_new_reshaped, y_new, epochs=5, batch_size=64, verbose=0)\n",
        "\n",
        "        # Predict on the new data\n",
        "        y_pred_scaled = model.predict(X_new_reshaped)\n",
        "\n",
        "        # Inverse transform to get original scale\n",
        "        y_pred_unscaled = scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "        # Optionally print a summary for each update\n",
        "        print(f'Update {epoch}: predictions (unscaled):', y_pred_unscaled)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "q7Qd_Wq2k20T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jiqiul0rSx8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics Evaluation**"
      ],
      "metadata": {
        "id": "DYZBMeli7zVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def evaluate_model_performance(ticker, y_test_unscaled, y_pred_unscaled):\n",
        "    # RMSE (Root Mean Square Error)\n",
        "    print(f'For {ticker}:')\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "\n",
        "    # MAE (Mean Absolute Error)\n",
        "    mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
        "    print(f\"MAE: {mae}\")\n",
        "\n",
        "    # Direction Accuracy\n",
        "    direction_accuracy = np.mean(\n",
        "        np.sign(y_test_unscaled[1:] - y_test_unscaled[:-1]) == np.sign(y_pred_unscaled[1:] - y_test_unscaled[:-1])\n",
        "    ) * 100\n",
        "    print(f\"Direction Accuracy: {direction_accuracy:.2f}%\")\n",
        "\n",
        "    # Backtesting\n",
        "    # Simulate trading: Buy if price is predicted to go up, sell if it goes down\n",
        "    returns = (y_test_unscaled[1:] - y_test_unscaled[:-1])  # Actual price changes\n",
        "    predicted_returns = (y_pred_unscaled[1:] - y_test_unscaled[:-1])  # Predicted price changes\n",
        "    profit = np.sum(np.sign(predicted_returns) * returns)\n",
        "\n",
        "    print(f\"Backtesting Profit: ${profit:.2f}\")"
      ],
      "metadata": {
        "id": "xhOFjsF471SV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot**\n"
      ],
      "metadata": {
        "id": "_qheOtQ446bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "def plot_actual_vs_predicted(data, y_test_unscaled, y_pred_unscaled, ticker):\n",
        "    # Plot the actual vs. predicted values on test data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Ensure you're plotting only the test range\n",
        "    plt.plot(data['Date'][-len(y_test_unscaled):], y_test_unscaled, label='Actual Closing Price', color='blue')\n",
        "    plt.plot(data['Date'][-len(y_test_unscaled):], y_pred_unscaled, label='Predicted Closing Price', color='green')\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Closing Price in US dollars ($)')\n",
        "    plt.title(f\"Actual vs Predicted Closing Prices of {ticker} (10 Years)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Set date format on x-axis (adapted for 10-year range)\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator(1))  # Tick every year\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Format ticks as 'YYYY'\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OXx87CKM45Iw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function**"
      ],
      "metadata": {
        "id": "Bxj7HE4SUdXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tickers and start date, end date is default today\n",
        "# tickers = ['AAPL', 'NVDA', 'MSFT', 'AMZN', 'GOOG', '2222.SR', 'META', 'TSLA', 'TSM', 'AVGO']\n",
        "tickers = ['AAPL']\n",
        "\n",
        "# tickers = ['']\n",
        "\n",
        "start_date = '2014-9-11'\n",
        "lstm_units = 50\n",
        "dropout_rate=0.2\n",
        "learning_rate=0.001\n",
        "epochs=10\n",
        "batch_size=64\n",
        "sequence_length=60 # Sequence length is consistent with the input shape\n",
        "input_shape=(60, 1)\n",
        "\n",
        "def lstm_model(ticker, start_date, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape):\n",
        "    # Download the data from yfinace\n",
        "    download_stock_data(tickers, start_date)\n",
        "    # print(data.tail())\n",
        "    # print(data.head())\n",
        "\n",
        "    # Scale data\n",
        "    data, close_prices_scaled, scaler = scale_stock_data(ticker)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "\n",
        "    # Create sequence for lstm\n",
        "    X_train, X_test, y_train, y_test = prepare_data_for_lstm(close_prices_scaled, sequence_length)\n",
        "\n",
        "    # Define and build the model\n",
        "    model, history, y_pred_unscaled, y_test_unscaled = build_and_train_model(X_train, y_train, X_test, y_test, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape)\n",
        "\n",
        "    # Show model information\n",
        "    show_model_info(model)\n",
        "\n",
        "    # Incremental Learning\n",
        "    updated_model = incremental_learning_process(ticker, model, scaler)\n",
        "\n",
        "    # Evaluate model\n",
        "    evaluate_model_performance(ticker, y_test_unscaled, y_pred_unscaled)\n",
        "\n",
        "    # Plot the model\n",
        "    plot_actual_vs_predicted(data, y_test_unscaled, y_pred_unscaled, ticker)\n",
        "    return\n",
        "\n",
        "for ticker in tickers:\n",
        "    lstm_model(ticker, start_date, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "P4o8sxAuUglR",
        "outputId": "cf16dca1-c1bf-4a99-d19c-bc2611ac142d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape and y shape: ((2523, 60, 1), (2523,))\n",
            "X_train shape and y_train shape: ((2018, 60, 1), (2018,))\n",
            "X_test shape and y_test shape: ((505, 60, 1), (505,))\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │          \u001b[38;5;34m10,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m91,955\u001b[0m (359.20 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">91,955</span> (359.20 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,651\u001b[0m (119.73 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,651</span> (119.73 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m61,304\u001b[0m (239.47 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,304</span> (239.47 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape\n",
            "\n",
            "(3, 60, 1)\n",
            "(3,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot take the length of shape with unknown rank.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1e228a6ac9dc>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-1e228a6ac9dc>\u001b[0m in \u001b[0;36mlstm_model\u001b[0;34m(ticker, start_date, lstm_units, dropout_rate, learning_rate, epochs, batch_size, input_shape)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Incremental Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mupdated_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincremental_learning_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-61ed9bbc3721>\u001b[0m in \u001b[0;36mincremental_learning_process\u001b[0;34m(ticker, model, scaler, iterations)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Train the model with verbose=0 to silence epoch printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Predict on the new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\u001b[0m in \u001b[0;36msqueeze_or_expand_to_same_rank\u001b[0;34m(x1, x2, expand_rank_1)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqueeze_or_expand_to_same_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_rank_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;34m\"\"\"Squeeze/expand last dim if ranks differ from expected by exactly 1.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mx1_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mx2_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx1_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx2_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot take the length of shape with unknown rank."
          ]
        }
      ]
    }
  ]
}